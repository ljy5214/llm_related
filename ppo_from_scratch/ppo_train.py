from transformers import AutoModelForCausalLM, AutoModel, AutoModelForSequenceClassification, AutoTokenizer
from dataclasses import dataclass
from typing import Optional, Union, Tuple
import random
import torch
import torch.nn.functional as F
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter

# æ„å»ºdataset
class PromptDataset(Dataset):
    def __init__(self, prompts, tokenizer, apply_chat_template=False):
        self.prompts = prompts
        self.tokenizer = tokenizer
        
        self.final_prompts = []
        
        for prompt in prompts:
            if apply_chat_template:
                content = [{"role": "user", "content": prompt}]
                prompt = self.tokenizer.apply_chat_template(content, tokenize=False, add_generation_prompt=True)
            else:
                prompt = self.tokenizer.bos_token + prompt
                
            self.final_prompts.append(prompt)
        
        
    def __len__(self):
        return len(self.prompts)
    
    def __getitem__(self, index):
        return self.final_prompts[index]

# ä»·å€¼ï¼ˆè¯„è®ºå®¶ï¼‰æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹æ¯ä¸€æ­¥ï¼ˆç”Ÿæˆtokenï¼‰çš„åŠ¨ä½œäº§ç”Ÿçš„æ”¶ç›Šï¼Œä½¿ç”¨æ¼”å‘˜æ¨¡å‹è¿›è¡Œåˆå§‹åŒ–ï¼Œå¹¶å¤–åŠ ä¸€ä¸ªå›å½’å¤´ï¼Œè¾“å‡ºshapeä¸ºï¼š(batch_size, seq_lenï¼Œ 1)
class Critic(nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model
        self.base_model.eval()
        self.value_head = nn.Linear(base_model.config.hidden_size, 1)
        
    def forward(self, input_ids, attention_mask, num_actions):
        
        
        hidden_state = self.base_model(input_ids, attention_mask=attention_mask).last_hidden_state # (batch_size, seq_len, hidden_size)
        value_model_output = self.value_head(hidden_state) #(batch_size, seq_lenï¼Œ 1)
        values = value_model_output.squeeze(-1)[:, -num_actions:] # (batch_size, seq_len)
        return values



def compute_policy_loss(log_probs, old_log_probs, advantages, action_mask=None, clip_eps=0.2):
    ratio = (log_probs - old_log_probs).exp()
    surr1 = ratio * advantages
    surr2 = ratio.clamp(1.0 - clip_eps, 1.0 + clip_eps) * advantages
    loss = -torch.min(surr1, surr2)
    if action_mask is None:
        return loss.mean(-1).mean()
    return ((loss * action_mask).sum(-1) / action_mask.sum(-1)).mean()


def compute_value_loss(values, old_values, returns, action_mask=None, clip_eps: float = None):
    if clip_eps is not None:
        values_clipped = old_values + (values - old_values).clamp(-clip_eps, clip_eps)
        surr1 = (values_clipped - returns) ** 2
        surr2 = (values - returns) ** 2
        loss = torch.max(surr1, surr2)
    else:
        loss = (values - returns) ** 2
        
    if action_mask is None:
        return loss.mean(-1).mean()
    return ((loss * action_mask).sum(-1) / action_mask.sum(-1)).mean()


class ExperienceBuffer:
    def __init__(self, limit):
        self.limit = limit
        self.buffer = []
    
    def append(self, experiences):
        batch = [{} for _ in range(len(experiences))]
        keys = (
        "seqs",
        "action_log_probs",
        "values",
        "returns",
        "advantages",
        "attention_mask",
        "action_mask",
        "num_actions"
    )
        for key in keys:
            for i, experience in enumerate(experiences):
                value = getattr(experience, key)
                batch[i][key] = value
          
        self.buffer.extend(batch)
        if len(self.buffer) >= self.limit:
            self.buffer = self.buffer[len(self.buffer)-self.limit:]
        
    def get_batches(self, batch_size):
        return random.sample(self.buffer, batch_size)
    
    def clear(self):
        self.buffer = []
        
    def __len__(self):
        return len(self.buffer)
    
    def __getitem__(self, index):
        return self.buffer[index]
    

@dataclass
class Samples:
    seqs: torch.Tensor
    attention_mask: Optional[torch.LongTensor]
    action_mask: Optional[torch.BoolTensor]
    num_actions: Union[int, torch.Tensor]
    packed_seq_lens: Optional[torch.Tensor]
    response_length: torch.Tensor
    total_length: torch.Tensor

@dataclass
class Experience:

    seqs: torch.Tensor
    action_log_probs: torch.Tensor
    values: torch.Tensor
    returns: Optional[torch.Tensor]
    advantages: Optional[torch.Tensor]
    attention_mask: Optional[torch.LongTensor]
    action_mask: Optional[torch.BoolTensor]
    reward: torch.Tensor
    response_length: torch.Tensor
    total_length: torch.Tensor
    num_actions: Union[int, torch.Tensor]
    kl: Optional[torch.Tensor] = None

def compute_approx_kl(
    log_probs: torch.Tensor,
    ref_log_probs: torch.Tensor,
    action_mask: Optional[torch.Tensor] = None,
):
    #	â€¢	log_probs: (B, T) â€”â€” æ¥è‡ª actor çš„é€ token å¯¹æ•°æ¦‚ç‡
	#â€¢	ref_log_probs: (B, T) â€”â€” æ¥è‡ªå‚è€ƒæ¨¡å‹çš„é€ token å¯¹æ•°æ¦‚ç‡
	#â€¢	action_maskï¼ˆå¯é€‰ï¼‰: (B, T)ï¼Œæˆ–ä»»ä½•å¯ä¸å‰ä¸¤è€…åœ¨æœ€åä¸€ç»´å¹¿æ’­å¯¹é½çš„å½¢çŠ¶ï¼ˆå¦‚ (B, T, 1)ï¼‰
    log_ratio = log_probs.float() - ref_log_probs.float()
    if action_mask is not None:
        log_ratio = log_ratio * action_mask

    return log_ratio #(B, T)

# A(t) = R(t) + gam*V(t+1) - V(t)
# gae:A(t) = R(t) + gam*V(t+1) - V(t) + gam*lam*A(t+1)
# æœ€åä¸€ä¸ªæ—¶åˆ»çš„æœªæ¥ä¼˜åŠ¿å’Œæœªæ¥æ”¶ç›Šä¸º0ï¼šA(T+1) = 0, V(T+1) = 0,  åˆ™A(T) = R(T) - V(T), å¾—å‡ºA(T)
# A(T-1) = R(T-1) + gam*V(T) - V(T-1) + gam*lam*A(T) çŸ¥é“A(T)å¯è®¡ç®—A(T-1) ä¾æ¬¡ç±»æ¨
# returns(t) = A(t) + V(t) = = R(t) + gam * (V(t+1) + lam * A(t+1))
def get_advantages_and_returns(
        values: torch.Tensor,
        rewards: torch.Tensor,
        action_mask: torch.Tensor,
        gamma: float,
        lambd: float):
    
    lastgaelam = 0
    advantages_reversed = []
    response_length = rewards.size(1)
    
    if action_mask is not None:
        values = action_mask * values
        rewards = action_mask * rewards

    for t in reversed(range(response_length)):
        nextvalues = values[:, t + 1] if t < response_length - 1 else 0.0
        delta = rewards[:, t] + gamma * nextvalues - values[:, t]
        lastgaelam = delta + gamma * lambd * lastgaelam
        advantages_reversed.append(lastgaelam)
    advantages = torch.stack(advantages_reversed[::-1], dim=1)
    returns = advantages + values
    return advantages.detach(), returns

def generate_samples(prompts, model, max_length, max_new_tokens, n_samples_per_prompt, micro_rollout_batch_size):
    samples_list = []
    model.eval()
    all_prompts = sum([[prompt]*n_samples_per_prompt for prompt in prompts], [])
    for i in range(0, len(all_prompts), micro_rollout_batch_size):
        prompts = all_prompts[i:i+micro_rollout_batch_size]
        inputs = actor_tokenizer(prompts, padding='max_length', max_length=max_length, truncation=True, return_tensors='pt')
        input_ids = inputs['input_ids']

        # è¿™é‡Œå¯èƒ½è¿˜éœ€è¦å†çœ‹çœ‹ï¼Œtransformersçš„æ¨ç†
        # è¿™é‡Œè¿”å›ç»“æœæ˜¯(B, M+T)åŒ…å«åŸå§‹ promptï¼ˆå‰ M ä½ï¼‰+ ç”Ÿæˆï¼ˆå T ä½ï¼‰ã€‚
        seqs = model.generate(**inputs.to(device), 
                            max_new_tokens = max_new_tokens, 
                            eos_token_id = eos_token_id, 
                            pad_token_id = pad_token_id)
        if seqs.size(1) >= max_new_tokens + max_length:
            seqs = seqs[:, :max_new_tokens + max_length]
        else:
            seqs = torch.cat([seqs, torch.full((seqs.size(0), max_new_tokens + max_length - seqs.size(1)), fill_value=pad_token_id, device=seqs.device)], dim=1)

        # è¿™é‡Œæ˜¯éœ€è¦è¯†åˆ«æ˜¯å¦æ˜¯paddingå­—ç¬¦ï¼Œeos_tokenå¹¶ä¸ç®—padding 
        attention_mask = (seqs.ne(pad_token_id)).to(dtype=torch.long)
        ans = seqs[:, input_ids.size(1):]
        # è¿™é‡Œè®¡ç®—çš„æ˜¯éœ€è¦è¿›è¡Œæ¢¯åº¦æ›´æ–°çš„åŠ¨ä½œï¼Œå› æ­¤eostokenå¹¶ä¸å‚ä¸åˆ°æ¢¯åº¦æ›´æ–°ä¸­
        action_mask = (ans.ne(eos_token_id) & ans.ne(pad_token_id)).to(dtype=torch.long)
       

        samples = Samples(
            seqs=seqs,
            attention_mask=attention_mask,
            action_mask=action_mask,
            num_actions=action_mask.size(1),
            packed_seq_lens=None,
            response_length=action_mask.float().sum(dim=-1),
            total_length=attention_mask.float().sum(dim=-1),
        )
        samples_list.append(samples)

    return samples_list


def compute_rewards(kl, r, action_mask, kl_ctl, clip_reward_value):

        # æŠŠklæ•£åº¦çš„è´Ÿæ•°ï¼Œåˆ†é…åˆ°å¥–åŠ±ä¸­
        # [bï¼Œacttion_num]
        kl_divergence_estimate = -kl_ctl * kl #é€å…ƒç´ ç›¸ä¹˜ â†’ kl_divergence_estimateï¼š(B, T)
        rewards = kl_divergence_estimate

        # (B,)ï¼Œè¡¨ç¤ºæ¯æ¡æ ·æœ¬æœ‰æ•ˆ token æ•° K
        ends = action_mask.sum(1) + 1
        if not isinstance(clip_reward_value, torch.Tensor):
            clip_reward_value = torch.tensor(clip_reward_value).to(r.device)

        # å¯¹ r åšé€å…ƒç´ æˆªæ–­ï¼ˆclampï¼‰ï¼ŒæŠŠå¥–åŠ±å€¼é™åˆ¶åœ¨ [-clip_reward_value, +clip_reward_value] åŒºé—´å†…ï¼Œé˜²æ­¢å¥–åŠ±è¿‡å¤§/è¿‡å°å¯¼è‡´ä¸ç¨³å®šï¼š
        # r : (B, 1)
        reward_clip = torch.clamp(r, -clip_reward_value,
                                  clip_reward_value)
        batch_size = r.size(0)
        for j in range(batch_size):
            # è¿™é‡Œåº”è¯¥æ˜¯åœ¨ã€Šeosã€‹è¿™ä¸ªtokenä¸ŠåŠ reward-modelçš„ç»“æœ
            rewards[j, :ends[j]][-1] += reward_clip[j, 0]

        return rewards

def generate_experiences(samples_list):

    actor_model.eval()
    ref_model.eval()
    reward_model.eval()
    critic_model.eval()

    experiences = []
    
    for samples in samples_list:
        seqs = samples.seqs
        attention_mask = samples.attention_mask
        action_mask = samples.action_mask
        num_actions = samples.num_actions
        with torch.no_grad():
            ## è®¡ç®—ç­–ç•¥æ¨¡å‹è¾“å‡ºtokençš„æ¦‚ç‡
            # seqs: [batch_size, seq_len]
            # è¿™é‡Œè°ƒç”¨çš„æ˜¯forwardå‡½æ•°ï¼Œè¿”å›ç»´åº¦ä¸ºã€batch_size, seq_len, vocab_sizeã€‘ï¼Œä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡
            # å¦‚æœæ˜¯generateï¼Œåˆ™æ˜¯åœ¨å†…éƒ¨è°ƒç”¨å¤šæ¬¡forwardï¼Œè¿”å›ç»“æœæ˜¯(BÃ—num_return_sequences, M+T)åŒ…å«åŸå§‹ promptï¼ˆå‰ M ä½ï¼‰+ ç”Ÿæˆï¼ˆå T ä½ï¼‰ã€‚
            output = actor_model(seqs, attention_mask=attention_mask)
            logits = output.logits # (B, L, V) 
 
            # éœ€è¦æ’é™¤æ‰æœ€åä¸€ä¸ªtoken
            log_probs = F.log_softmax(logits[:, :-1, :], dim=-1) #(B, L-1, V) 
            # å‘åå–ä¸€ä½
            # seqs[:, 1:].unsqueeze(-1) â‡’ (B, L-1, 1)ï¼Œä¸”å…ƒç´ æ˜¯ token idï¼ŒèŒƒå›´åœ¨ [0, V-1]
            # log_probs_labels[b, t, 0] = log_probs[b, t, y[b, t]]
            # æ¯ä¸€ä¸ªtokenå¯¹åº”çš„labelï¼ˆä¸‹ä¸€ä¸ªtokenï¼‰çš„æ¦‚ç‡
            log_probs_labels = log_probs.gather(dim=-1, index=seqs[:, 1:].unsqueeze(-1)) #(B, L-1, 1)
            action_log_probs = log_probs_labels.squeeze(-1)[:, -num_actions:]

            ##è®¡ç®—å‚è€ƒæ¨¡å‹è¾“å‡ºtokençš„æ¦‚ç‡
            ref_output = ref_model(seqs, attention_mask=attention_mask)
            ref_logits = ref_output.logits
            ref_log_probs = F.log_softmax(ref_logits[:, :-1, :], dim=-1)
            ref_log_probs_labels = ref_log_probs.gather(dim=-1, index=seqs[:, 1:].unsqueeze(-1))
            ref_action_log_probs = ref_log_probs_labels.squeeze(-1)[:, -num_actions:]

            ## è®¡ç®—ä»·å€¼
            # batch_size, seq_len
            value = critic_model.forward(seqs, attention_mask, num_actions).to(device)
            # è½¬æ¢æˆæ–‡æœ¬
            seq_texts = actor_tokenizer.batch_decode(seqs, skip_special_tokens=True)
            # è®¡ç®—å¥–åŠ±æ¨¡å‹çš„å¥–åŠ±å€¼ 
            # è¿™é‡Œéœ€è¦å…ˆdecodeå†encodeï¼Œæ˜¯å› ä¸ºreward modelå’Œactor modelçš„è¯è¡¨å¯èƒ½ä¸å¤ªä¸€æ ·
            reward_model_inputs = reward_tokenizer(seq_texts, return_tensors="pt", padding=True)

            # ç»´åº¦çŒœæµ‹æ˜¯ ã€bï¼Œ1ã€‘
            r = reward_model(**reward_model_inputs.to(device)).logits # å¥–åŠ±æ¨¡å‹çš„è¾“å‡ºï¼Œç›¸å½“äºç”Ÿæˆæœ€åä¸€ä¸ªtokençš„å¥–åŠ±ï¼ˆç»“æœå¥–åŠ±æ¨¡å‹ï¼‰
            # è®¡ç®—klæ•£åº¦

            # ç»“æœæ˜¯ [bï¼Œacttion_num]
            kl = compute_approx_kl(
                    action_log_probs,
                    ref_action_log_probs,
                    action_mask=action_mask).to(device)
            # è®¡ç®—å®é™…å¥–åŠ±
            rewards = compute_rewards(kl, r, action_mask, kl_ctl=0.1, clip_reward_value=0.2)
            # è®¡ç®—ä¼˜åŠ¿å’Œå›æŠ¥
            advantages, returns = get_advantages_and_returns(value, rewards, action_mask, gamma=0.1, lambd=0.2)
        # actor_model.train()
        # critic_model.train()

        experiences.append(Experience(seqs,
                    action_log_probs.detach(),
                    value.detach(),
                    returns.detach(),
                    advantages.detach(),
                    attention_mask,
                    action_mask,
                    r.detach(),
                    samples.response_length,
                    samples.total_length,
                    num_actions,
                    kl.detach(),
        ))

    return experiences

@dataclass
class BufferItem:

    seqs: torch.Tensor
    action_log_probs: torch.Tensor
    values: torch.Tensor
    returns: torch.Tensor
    advantages: torch.Tensor
    attention_mask: torch.Tensor
    action_mask: torch.Tensor
    num_actions: Union[int, torch.Tensor]

def collate_fn(batch):

    seqs = []
    action_log_probs = []
    values = []
    returns = []
    advantages = []
    attention_mask = []
    action_mask = []
    
    for x in batch:
        seqs.append(x['seqs'])
        action_log_probs.append(x['action_log_probs'])
        values.append(x['values'])
        returns.append(x['returns'])
        advantages.append(x['advantages'])
        attention_mask.append(x['attention_mask'])
        action_mask.append(x['action_mask'])

    seqs = torch.cat(seqs, dim=0)
    action_log_probs = torch.cat(action_log_probs, dim=0)
    values = torch.cat(values, dim=0)
    returns = torch.cat(returns, dim=0)
    advantages = torch.cat(advantages, dim=0)
    attention_mask = torch.cat(attention_mask, dim=0)
    action_mask = torch.cat(action_mask, dim=0)
    
    return BufferItem(seqs, action_log_probs, values, returns, advantages, attention_mask, action_mask, action_mask.size(1))
    
def train_step(experience, steps):
    
    actor_model.train()
    optimizer_actor.zero_grad()

    
    sequences = experience.seqs
    old_action_log_probs = experience.action_log_probs
    advantages = experience.advantages
    num_actions = experience.num_actions
    attention_mask = experience.attention_mask
    action_mask = experience.action_mask
    old_values = experience.values
    returns = experience.returns
    
    logits = actor_model(
            sequences,
            attention_mask=attention_mask).logits
    
    log_probs = F.log_softmax(logits[:, :-1, :], dim=-1)
    log_probs_labels = log_probs.gather(dim=-1, index=sequences[:, 1:].unsqueeze(-1))
    action_log_probs = log_probs_labels.squeeze(-1)[:, -num_actions:]
  

    
    policy_loss = compute_policy_loss(action_log_probs, old_action_log_probs, advantages,action_mask=action_mask)
    policy_loss.backward()
    optimizer_actor.step()  
    writer.add_scalar("policy_loss", policy_loss.item(), steps)
    
    critic_model.train()
    optimizer_critic.zero_grad()
    values = critic_model.forward(sequences, attention_mask, num_actions)
    value_loss = compute_value_loss(values, old_values, returns, action_mask)
    value_loss.backward()
    optimizer_critic.step()
    writer.add_scalar("value_loss", value_loss.item(), steps)
    print(f"step: {steps}  policy_loss: {policy_loss.item():.4f}  value_loss: {value_loss.item():.4f}")
    

def train():
    # åˆå§‹åŒ–ç»éªŒæ± 
    buffer = ExperienceBuffer(limit=100)
    steps = 0
    for episode in range(episodes):
        for rand_prompts in prompts_dataloader:
            # è¿™é‡Œå–rollout_batch_sizeä¸ªæ ·æœ¬
            # ç”Ÿæˆæ ·æœ¬ï¼ˆè·å–æ¨¡å‹æ¨ç†ç»“æœï¼‰
            samples = generate_samples(rand_prompts, actor_model, max_length, max_new_tokens, n_samples_per_prompt, micro_rollout_batch_size)
            # ç”Ÿæˆç»éªŒï¼ˆè·å–ä¼˜åŠ¿ã€å¥–åŠ±ã€å›æŠ¥ç­‰ï¼‰
            experiences = generate_experiences(samples)
            buffer.append(experiences)
            dataloader = DataLoader(buffer, batch_size=micro_train_batch_size, shuffle=True, collate_fn=collate_fn)
            torch.cuda.empty_cache()
            for epoch in range(max_epochs):
                for experience in dataloader:
                    train_step(experience, steps)
                    steps += 1
            
            buffer.clear()
        
            torch.cuda.empty_cache()
            

if __name__ == "__main__":
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    # ä¸€å…±è¿­ä»£å¤šå°‘è½®
    episodes = 3
    # ç”Ÿæˆä¸€æ¬¡ç»éªŒï¼Œè®­ç»ƒçš„è½®æ•°
    max_epochs = 5
    # ä¸€æ¬¡ä»æç¤ºè¯æ•°æ®é›†ä¸­å–å¤šå°‘æ¡æ•°æ®ç”¨äºç”Ÿæˆç»éªŒ
    rollout_batch_size = 8
    # ä¸€æ¬¡å–å¤šå°‘æ¡æ•°æ®ç”Ÿæˆç»éªŒï¼ˆç”Ÿæˆç»éªŒéœ€è¦å¤šä¸ªæ¨¡å‹æ¨ç†ï¼Œå¯¹æ˜¾å­˜è¦æ±‚é«˜ï¼‰
    micro_rollout_batch_size = 2
    # ä¸€ä¸ªæç¤ºè¯ç”Ÿæˆå¤šå°‘ä¸ªæ ·æœ¬
    n_samples_per_prompt = 2
    # ç”Ÿæˆçš„æœ€å¤§é•¿åº¦ï¼Œç›¸å½“äºæœ€å¤§åŠ¨ä½œæ•°ï¼Œæ•°å€¼è¶Šå¤§ï¼Œæ¨¡å‹æ¢ç´¢çš„å¯èƒ½æ€§è¶Šå¤š
    max_new_tokens = 50
    # æœ€å¤§é•¿åº¦
    max_length = 256
    # å®é™…è®­ç»ƒçš„batch_sizeå¤§å°ï¼Œä¸€æ¬¡å–å¤šå°‘æ¡æ•°æ®ç”¨äºæ›´æ–°å‚æ•°
    micro_train_batch_size = 2
    # è®°å½•æ—¥å¿—
    writer = SummaryWriter('./runs')
    # ç­–ç•¥æ¨¡å‹
    actor_model = AutoModelForCausalLM.from_pretrained('/home/user/Downloads/Qwen2.5-0.5B-Instruct').to(device)
    # å‚è€ƒæ¨¡å‹
    ref_model = AutoModelForCausalLM.from_pretrained('/home/user/Downloads/Qwen2.5-0.5B-Instruct').to(device)
    # å¥–åŠ±æ¨¡å‹
    reward_model = AutoModelForSequenceClassification.from_pretrained('/home/user/Downloads/reward-model-deberta-v3-large-v2').to(device)
    actor_tokenizer = AutoTokenizer.from_pretrained('/home/user/Downloads/Qwen2.5-0.5B-Instruct')
    reward_tokenizer = AutoTokenizer.from_pretrained('/home/user/Downloads/reward-model-deberta-v3-large-v2')
    # ä»·å€¼æ¨¡å‹

    #	ä½ ä¼ å…¥çš„æ˜¯ actor_model.base_modelã€‚åœ¨ ğŸ¤—Transformers é‡Œï¼ŒAutoModelForCausalLM ç”±ä¸¤éƒ¨åˆ†æ„æˆï¼š
	# base_modelï¼šTransformer ä¸»ä½“ï¼Œè¾“å‡º last_hidden_state âˆˆ â„^{BÃ—LÃ—H}ï¼›
	# lm_headï¼šæŠŠéšè—ç»´ H æŠ•åˆ°è¯è¡¨ç»´ V çš„çº¿æ€§å±‚ï¼ˆæˆ– MLPï¼‰ï¼Œè¾“å‡º logitsã€‚
    critic_model = Critic(actor_model.base_model).to(device)
    
    # åˆå§‹åŒ–ä¼˜åŒ–å™¨
    optimizer_actor = torch.optim.Adam(actor_model.parameters(), lr=0.00005)
    optimizer_critic = torch.optim.Adam(critic_model.parameters(), lr=0.00005)
    
    # å¡«å……æ–¹å¼ä¸ºå·¦å¡«å……
    actor_tokenizer.padding_side = 'left'
    eos_token_id = actor_tokenizer.eos_token_id
    pad_token_id = actor_tokenizer.pad_token_id
    prompt_list = [
        'è¯·é—®1+1ç­‰äºå¤šå°‘ï¼Ÿ',
        'PowerShellï¼Œå¦‚ä½•çŸ¥é“BIOSä¸­çš„è™šæ‹ŸåŒ–æ˜¯å¦å·²ç¦ç”¨',
        'ä¸ºä»€ä¹ˆäººä»¬å–œæ¬¢åœ¨æ°´æ—é¦†é‡Œæ¸¸æ³³ï¼Œè€Œä¸æ˜¯åœ¨æ¸¸æ³³æ± é‡Œï¼Ÿ',
        'ä½ æ˜¯ä¸€ä½è¥é”€ä¸“å®¶ã€‚ä¸ºInstagram reelså†™30ä¸ªå¸¦æœ‰è¥é”€æŠ€å·§çš„è„šæœ¬ã€‚',
        'ä½ æ˜¯ä¸€ä½è¥é”€ä¸“å®¶ã€‚ä¸ºInstagram reelså†™30ä¸ªå¸¦æœ‰è¥é”€æŠ€å·§çš„è„šæœ¬ã€‚',
        'ä½ æ˜¯ä¸€ä½è¥é”€ä¸“å®¶ã€‚ä¸ºInstagram reelså†™30ä¸ªå¸¦æœ‰è¥é”€æŠ€å·§çš„è„šæœ¬ã€‚',
        'ä¸ºä»€ä¹ˆæ‰€æœ‰çš„é•œå­éƒ½æ˜¯çŸ©å½¢çš„ï¼Ÿ',
        'æˆ‘ä»¬åœ¨å—æ„ŸæŸ“çš„æ¤ç‰©æ ¹éƒ¨å¯ä»¥æ‰¾åˆ°å“ªä¸€ç§ï¼Œè‡­æ°§è¿˜æ˜¯é‡‘å­ï¼Ÿ'
    ]
    prompts_dataset = PromptDataset(prompt_list, actor_tokenizer, apply_chat_template=True)
    prompts_dataloader = DataLoader(prompts_dataset, batch_size=rollout_batch_size, shuffle=True)
   
    train()
    

